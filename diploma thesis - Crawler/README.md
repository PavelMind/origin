## Дипломный проект "Поисковая система"
#### Описание задания
Необходимо написать поисковую систему — аналог Google, Яндекс или Yahoo.  
Поисковая система будет получать данные с сайтов, строить поисковые индексы и по запросу выдавать релевантные результаты поисковой выдачи.

Поисковая система будет состоять из следующих частей:  
- программа «Паук». Это HTTP-клиент, задача которого — парсить сайты и строить индексы исходя из частоты слов в документах;
- программа-поисковик. Это HTTP-сервер, задача которого — принимать запросы и возвращать результаты поиска.  

Для хранения информации следует использовать базу данных PostgreSQL. Настройки программ следует хранить в ini-файле конфигурации.
### Программа "Паук" (spider), или "Краулер" (crowler)
Эта программа, которая обходит интернет, следуя по ссылкам на веб-страницах. Начиная с одной страницы, она переходит на другие по ссылкам, найденным на этих страницах. «Паук» загружает содержимое каждой страницы, а затем индексирует её.
#### Индексация
Индексатор выполняет следующие действия для страницы:
- страница очищается от HTML-тегов, от знаков препинания, табуляции, переносов строк и т. д. Остаются только слова, разделённые пробелами;
- слова переводятся в нижний регистр. Для простоты можно также отбрасывать все слова короче трёх или длиннее 32 символов;
- считает, сколько раз встречается каждое слово в тексте; информация о словах и о частотности сохраняется в базу данных.
#### Переход между страницами
В ini-файле конфигурации задаётся страница, с которой «Паук» начинает свою работу.

"Паук"("краулер") должен переходить по html-ссылкам и скачивать эти страницы тоже. Для этого «Паук» реализует многопоточную очередь и пул потоков. Очередь состоит из ссылок на скачивание, а пул потоков выполняет задачу по скачиванию веб-страниц с этих ссылок и их индексацию. После скачивания очередной страницы «Паук» собирает все ссылки с неё и добавляет их в очередь на скачивание. Глубина рекурсии переходов должна быть задана настройкой в ini-файле конфигурации в виде числа. Например, при глубине рекурсии в 1, "Паук" анализирует только стартовую страницу.
### Программа-поисковик
Программа-поисковик выполняет поиск по базе данных, ранжирует результат и возвращает его пользователю. 

Изнутри поисковик представляет из себя HTTP-сервер, который принимает два вида запросов: POST и GET. Порт, на котором будет запущен HTTP-сервер, должен взять из ini-файла конфигурации.  
По запросу GET открывается простая статическая HTML-страница с формой поиска. На форме должно быть поле ввода, а также кнопка поиска.  
По запросу POST происходит извлечение из базы данных результата запроса и его ранжирование, а затем перед пользователем открывается простая статическая веб-страница с результатами поиска.

Для извлечения данных запрос должен быть следующим: взять документы, в которых встречаются все слова из запроса, и отсортировать их по суммарному количеству упоминаний слов в документе.

В случае, если при работе программы возникла ошибка, или если результатов не найдено, на HTML-странице следует отобразить соответствующее сообщение.
### Реализация
В программе используются библиотеки: Boost (:locale для изменений регистра), OpenSSL, cpp-httplib, libpq++. К сожалению, не нашёл хороший способ преобразований кодировок, поэтому поисковик работает только с англоязычными сайтами.

В обоих программах (Crawler и Searcher) создан класс DBclass, содержащий подключение к базе данных и использующий библиотеку libpq++. Метод getConn() возвращает ссылку на подключение и используется для создания транзакций. В папке "sql builders" созданы классы, использующиеся для построения SQL-запроса. Они используют паттерн "строитель". 

Класс addrSite содержит URL-адрес страницы в интернете. Он хранит хост, путь страницы и тип протокола(для работы нам важны только http и https).  
Метод ref() возвращает true, если адрес указывает именно на страницу сайта.  
Его конструкторы:
- addrSite (bool, std::string, std::string) - формирует URL-адрес по отдельным частям;
- explicit addrSite (std::string) - получает строку с абсолютным адресом и считывает из неё;
- addrSite (std::string, const addrSite) - первым аргументом получает URL-адрес, который может быть абсолютным или относительным. Если он относительный, то вторым аргументом передаётся объект адреса, к которому первый аргумент относится. Иначе второй игнорируется.  

Класс парсера ini-файлов идентичен классу в [курсовой работе](https://github.com/PavelMind/origin/tree/main/advanced%20programming/Parser%20INI-files/task)
#### Только в Crawler
Класс listLinks - общий список ссылок, находимых индексатором на html-странице. Также содержит множество уже посещённых адресов. Методы push() и empty() работают со списком ссылок, которые надо будет посетить, getLinks() возвращает все накопленные. Метод isProcAndMarked() получает адрес и, если он не был ранее обработан, отмечает его посещённым и возвращает false.

Класс HTTPclient используется для создания TCP-клиента, который будет подключаться к сайтам. В нём используется библиотека cpp-httplib. Так как в интернете в основном используется протокол https с защитой SSL, его поддержку надо включить через `#define CPPHTTPLIB_OPENSSL_SUPPORT`. Класс получает хост при инициализации. Метод Get() получает в аргументе путь страницы и возвращает её html-текст.

Класс indexator в методе indexation() вызывает cleanerText(), очищающий полученный html-текст от всего ненужного и приводит к нижнему регистру, попутно записывающий заголовок страницы и заполняющий список ссылок в классе listLinks. После этого составляет список всех слов в тексте и записывает в методе inputBD() их в базу данных. Знаю, что эффективнее было бы сразу в cleanerText() формировать список слов, но решил делать как сказано в задании.

В программе Crawler основную работу выполняет класс iterateSite. В главной функции программы scanning() стартовая страница добавляется в список на скачивание, после чего в цикле в количестве заданной рекурсии для каждого адреса запускается функция work(), которая отправляется в пул потоков. В work() скачивается страница и индексируется. При индексации listLinks заполняется ссылками из html-текста сайта, и на новой итерации программа будет загружать уже их.  
Индексатор собирает статистику изменений в базе данных, которую предоставляет методом getAdded(). Статистика выводится в конце работы scanning().

Файл crawler_data.ini содержит данные для подключения к базе данных, адрес стартового сайта, количество рекурсии ("глубины") переходов по ссылкам на страницах, и порт для TCP-клиента. black_list_site.txt содержит хосты, по ссылкам на которые не будет переходов.
#### Только в Searcher
Работа проводится в классе HTTPserver. Он создаёт TCP-сервер с помощью библиотеки cpp-httplib. Для тестирования объект сервера создаётся с конструктором, предполагающим открытие хоста "localhost", что позволяет открывать наш сайт у себя на устройстве. Порт читается из ini-файла. Для открытия у себя на устройстве используется http (без SSL).  

В конструкторе устанавливаются обработчики для получения сообщений от браузера клиента на пути. Сообщение GET на адрес хоста возвращает страницу, находящуюся в файле main_crawler.html. Сообщение POST принимается по нажатию кнопки "поиск" на главной странице и несёт строку, написанную пользователем в текстовом поле. Обработчик этого сообщения - метод createListResp().

createListResp() приводит запрос к нижнему регистру и выделяет из него слова в createListWords(). Эти слова передаются в reqFromDB(), который делает SQL-запрос в базу данных и возвращает список заголовков(возможно, пустой) и адресов. Далее формируется html-страница ответа, отправляемая на браузер клиента.